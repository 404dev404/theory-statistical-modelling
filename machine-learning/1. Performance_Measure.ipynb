{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much better way to evaluate the performance of a classifer is to look at the _confusion matrix_. The general idea is to count the number of times instances of class A are classified as class B. For Instance, to know the number of times the classifer confused an MNIST image of 5s with 3s, you would look in the 5th row and the 3rd column of the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://www.dataschool.io/content/images/2015/01/confusion_matrix2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aformentioned __confusion matrix__ tells us that the model was successful in predicting 50 instances where the model was a true negative. Furthermore, that it was successful in predicting 100 instances of true positves. However, the model appears to be inaccurate in classifing 5 instances where the expected prediction was a positive, but the model provided a false estimation of a netgative. This can be similarily found for FP, where the computer estimated a positive, but the actual expected result was a negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concise Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the confusion matrix provides a lot of information, sometimes it is more preferable to view more concise metrics. An interesting one to look at is the accuracy of a positive prediction, this is called a _precision_ of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Equation of Precision can be calculated as:\n",
    "\n",
    " $$ Precision = {\\frac {TP}{TP + FP}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP is the number of true positives, and FP is the number of false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the ratio of positive instances that are correctly detected by the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Recall = {\\frac {TP} {TP + FN}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where FN is the number of False negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 measure is a combination of precision and recall into a single metric, in particular if you need a simple way to compare two classifiers. The F1 score is the harmonic mean of precision and recall. Whereas the regular mean treats all values equally. The F1 score favors classifiers that have similar precision and recall. For example, if you trained a classifier to detect videos that are safe for kids, you would probably prefer a classifer that rejects many good videos(low recall) but high precision (high precision), rather than a classifier that has a much higher recall but lets a few really bad videos show up in your products. On the otherhand, suppose you train a classifer to detect shopliferts on surveillance images: it is probably find if your classifer has only a 30% precision, as long as it has a 99% recall (sure, the security guards will get a few false alerts, but almost all shoplifters will get caught)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
